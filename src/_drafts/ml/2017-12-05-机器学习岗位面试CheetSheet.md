---
layout:  post
title: "机器学习岗位面试 Cheat sheet" 
date:   2019-08-11 22:47:42                    
author:  "tzuw"
header-img: "img/post-bg-2015.jpg"
catalog:   false
tags: 機器學習
---
>> My Cheet sheet
>
> 高频话题：SVM、Logistic Regression、决策树、随机森林、聚类算法

### 数学基础

1. 如何判断函数凸或非凸？

   两种办法：

   - 如果一个函数的二阶导数大于等于零，那么他就是凸函数
   - 分析目标函数的结构（通过记住一些常见函数的特性）
     - 指数函数是凸函数；对数函数是凹函数
     - 对一个凸函数进行线性变换，结果还是凸函数
     - 二次函数的二次项系数为正时，是凸函数
     - 高斯分布是凹函数

2. 解释一下对偶的概念

   线性规划中，如果原始问题约束条件多，决策变量少，转换为对偶问题约束变少了，更好求解。

### 机器学习算法知识

1. **无监督/有监督算法区别？**

   有没有标签。

2. **SVM的推导与特性？多分类时怎么处理？**

   推导都见另一份笔记。

   SVM模型的基本定义，寻找特征空间上间隔最大的分割超平面。可以说，它是特征空间上间隔最大的线性分类器，那么他的学习策略就是间隔最大化。

   如何求解？

   由于目标函数是一个二次函数，且他的约束条件是线性的，所以这个问题是一个凸二次规划问题。周志华老师的西瓜书上使用，把每个约束条件乘上一个拉格朗日乘子加入目标函数中，得到原来目标函数（最大化间隔）的“对偶问题”，通过求解对偶问题得到原问题的最优解。

   > 线性规划中，如果原始问题约束条件多，决策变量少，转换为对偶问题约束变少了，更好求解。

3. **解释下Logistic regression？**

   > ​
   >
   > 思想：先用参数表达后验概率，直接从训练集估计后验概率***P(Y|X)*** ，是一个判别式模型。
   >
   > 模型基本可以表示为：<img src="../pics/logistic-regression-model.png" height="40px"> （1）
   >
   > 公式（1）中右式表示的是“几率”，某个样本点x作为正例的相对可能性，这一个步主要是想使用参数来表达训练集中样本的后验概率值。
   >
   > 取对数后，可以分别得到**<u>*后验概率的参数表达式*</u>**：
   >
   > <img src="../pics/logistic-substitution-1.png" height="50px">(2)
   >
   > <img src="../pics/logistic-substitution-2.png" height="40px">(3)
   >
   > 公式（2）（3）中，参数**x, b**表示的是向量。
   >
   > ---
   >
   > 周志华老师书上说：“通过*极大似然估计*来估计参数*w*和*b*“ **就是把逻辑斯蒂回归的表达式写成似然函数，然后最大化似然函数，得到参数的最优值。**于是，似然函数<img src="../pics/logistic-regression.png" height="50px">（4）
   >
   > 公式（4）中，m是样本个数；ln和sigma符号是取了对数后产生的，原本应该是一个连乘的概率；最后<img src="../pics/logistic-regression-posterior-prob.png" height="25px">代表的是后验概率。最大化似然函数 <u>意即，让每一个样本的后验概率最大。</u>  <u>也意即 ，让训练集中每一个样本被分类器判断是其真实标记的概率最大。</u>
   >
   > ---
   >
   > 推导继续：
   >
   > 上面的后验概率可以被分开表示为每个样本属于其真正标记的概率，<img src="../pics/logistic-reg-tuidao.png" height="60px">（5）
   >
   > 到这里都还可以很简单，但是后面就需要用到<u>模型的基本形式</u>来代换上面的后验概率，才能完成最后的损失函数形式。~~**所以，逻辑斯蒂回归的精髓难道在用参数来表达训练集中样本的后验概率值**。~~~~天啊，突破盲点。~~  
   >
   > <img src="../pics/logistic-reg-tuidao-1.png" height="50px">(6)
   >
   > <img src="../pics/logistic-reg-tuidao-2.png" height="50px">(7)
   >
   > 最后，通过*梯度下降法*求解参数。主要要将（7）式加一个负号。
   >
   > ​

4. **决策树的特性？**

   决策树学习的目的是产生一颗泛化能力强的决策树，它的基本流程遵循简单的“分而治之”策略。

   用浅显易懂的话概括地说：

   步骤如下：

   （1）递归停止条件

   （2）从属性集中选择**最优分裂属性**

   （3）使用该属性进行分裂，得到对应的子节点

   （4）对得到的子节点中的数据呼叫同样的函数，实现递归；直到满足递归停止条件为止

   *<u>递归停止条件</u>*：属性集中没有属性；属性集中没有可以降低信息熵的属性；节点中只含有一类样本；节点只含有一个或零个样本；节点中的样本在所有余下属性的取值都相同。

   *<u>划分规则</u>*：（用一句话和浅显的公示表示）

   - 信息增益
     - 使用某个属性分裂前后的信息熵大小
     - 公式：<img src="../pics/Entropy.png" height="40px">；其中，k表示某个属性取值的个数，pk表示某类样本(y)在属性a取值为XX时，占当前节点所有样本的比率
     - 信息熵越小，代表样本越纯
     - 信息增益指的就是属性分裂前后的信息熵大小
   - 信息增益率
     - 公式：<img src="../pics/Entropy-Gain-ratio.png" height="40px">；其中，<img src="../pics/Entropy-gainRatio-intrinsicValue.png" height="40px">称为某个属性a的固有值。(Intrinsic Value)
   - 基尼指数
     - CART决策树就是使用基尼指数来进行选择划分属性的，可以说基尼指数要优于前面两种划分方法。为什么？
     - 公式：某个数据集的纯度度量；<img src="../pics/gini-index.png" height="40px">其中，k表示某个属性的取值个数，pk表示某类样本(y)在属性a取值为XX时，占当前节点所有样本的比率

   *<u>处理决策树的过拟合问题</u>* :前剪枝；后剪枝

5. **决策树、LR、SVM对比**

   > Logistic Regression与Support Vector Machine的异同？

6. **GBDT和随机森林的区别？**

   简而言之，详细可以写另一篇笔记了。涉及到**boosting**和**bagging**两个思想。

   - Bootstraping（自助法），源自于"pull up by your own bootstraps"。意思是，依靠你自己的资源。一种有放回的抽样方法（可能会抽到重复的样本）。

   - Boosting：其主要思想是将弱分类器组装成一个强分类器。举例来说，Adaboost算法，在训练每个弱分类器时，提高了之前分类错误的数据的权重值，从而训练出来一组能够互补的弱分类器。

     > 两个核心问题：
     >
     > A)  在每一轮中如何改变**训练数据的权值**？
     >
     > 通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。
     >
     > B）通过什么方式来组合弱分类器？
     >
     > 通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。

   - Bagging（Bootstrap AGGregatING）（套袋法）

     > 步骤：
     >
     > （A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
     >
     > （B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。
     >
     > （C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

     <!--两者的区别与比较-->

   - Random Forest

     以随机的方式建立一个森林，在森林中建立每一颗决策树时，有两个地方需要注意，第一，采样方法；第二，完全分裂。采样方法指的是，建立每颗决策树时，属性集随机抽取但不抽取完，样本随机抽取但不抽取完，并且，所有的采样抽取都是有放回采样，也就是说会抽取到一样的样本，选到相同的属性集合。完全分裂，指的是不采用剪枝的方法来克服过拟合；相对的在随机森林，使用随机采样多次的方式来克服过拟合问题。

     简单地说，随机森林就是训练很多棵决策树，然后通过投票决定最后的分类结果。

   - Gradient Boosting Decision Tree

     ~~其中，Boosting是提升的意思，所以可以这样记：GBDT的每一颗决策树都是为了提升上一颗决策树的分类结果。（每一次的训练是为了提升上一次的结果）~~错错错

     ~~**最简单的Boosting**：~~

     - ~~初始化为每一个样本赋上一个权重值，大家的权重值相同，也就是每个样本都是一样重要的。~~
     - ~~在每一步训练中得到的模型，会使得数据点的估计有对有错。于是，在每一步结束后，增加分错的点的权重，减少分对的点的权重，这样使得某些点如果老是被分错，那么就会被“**严重关注**”，也就被赋上一个很高的权重。~~
     - ~~最后，进行了N次迭代，得到N个弱分类器，将它们组合起来（比如说可以对它们进行加权、或者让它们进行投票等），得到最终的模型。~~

     ~~**Gradient Boosting**：~~

     - ~~每一次的训练是为了减少上一次的残差。~~
     - ~~每一次建立模型是在上一个模型的损失函数的梯度下降方向。~~
     - ~~有效地说，损失函数描述的是模型的犯错误比率，若损失函数越小，那么模型准确率越高，于是，将建立模型的方式定为：使得损失函数在梯度方向上下降，就可以说是模型在不断的改进。~~

7. **介绍一下卷积神经网络** Convolutional Neural Network

   一句话概括，恩。就是现在在语音识别和图像识别运用的很好的算法。	在图像领域应用的很好，概括地说是因为，第一，卷积神经网络，“卷积”是对图片上一小块像素区域进行处理，相比较对单一像素点处理，能够加深模型对图片的理解；第二，CNN可以有效的**减少参数个数**，使得训练变得可行。（减少参数的方法？*局部感受*；*参数共享*）

8. **卷机神经网络和DBN有什么区别？**

   ~~DBN（Deep Belief Network）深度信念网络。~~

   ~~深度信念网络，是一个概率生成模型。与传统的判别模型的神经网络相对，生成模型是建立一个观察数据和标签之间的联合分布，对*P(Observation|Label)*和 *P(Label|Observation)*都做了评估，而判别模型仅仅而已评估了后者，也就是*P(Label|Observation)*。~~

9. **解释贝叶斯公式；解释朴素贝叶斯分类**

   - 贝叶斯公式：<img src="../pics/bayes-rules.png" height="40px">；给定一组所关心事件的先验概率，如果你收到新的信息，那么更新对事件发生概率的估计。
   - 朴素贝叶斯分类最重要的就是“属性独立性假设”，即假设属性间相互独立。

10. **介绍一下EM算法   Expectation-Maximization 最大期望算法**

  EM算法可以概括為兩個步驟的交替執行兩步，：

  **E步**：（Expectation）根据参数初始值或上一次迭代的模型参数值来计算出隐性变量的后验概率，也就是先估计一下隐形变量的现有估计值。

  **M步**：（Maximization）最大化似然函数从而获得新的参数值。

11. **举例两个使用EM算法的模型，举例说明；为什么这些模型要使用EM算法，而不使用牛顿法或者梯度下降法？**

    混合高斯模型、K-means聚类等。

    因为有属性变量未知，难以直接求解，西瓜书上提到：*EM算法可看做用<u>坐标下降算法</u>，来最大化对数似然下界的过程。*

### 模型选择

1. 什么是过拟合，为什么会产生？


   >    训练数据上表现良好，但是测试数据上表现差。简单来说，样本数据非常好，为观测数据拟合差。
   >
   >    **产生原因**：
   >
   >    （A）建模样本抽取错误，包括（但不限于）样本数量太少，抽样方法错误，抽样时没有足够正确考虑业务、场景或业务特点，等等导致抽出的样本数据不能有效足够代表业务逻辑或业务场景
   >
   >    （B）建模时的“逻辑假设”到了模型应用时已经不能成立了
   >
   >    **（D）<u>样本里的噪音数据干扰过大，大到模型过分记住了噪音特征；反而忽略真实的输入输出间的关系</u>**
   >
   >    **（E）<u>建模时使用了太多的输入变量，或者训练数据过少</u>**
   >
   >    **举例**：决策树模型搭建中，如果我们对于决策树的生长没有合理的限制和修剪的话，决策树的自由生长有可能每片叶子里只包含单纯的事件数据(event)或非事件数据（no event），可以想象，这种决策树当然可以完美匹配（拟合）训练数据，但是一旦应用到新的业务真实数据时，效果就会一塌糊涂

2. 有哪些方法可以预防或者克服过拟合？

   > （A） 增大数据量
   > （B） 减少feature个数（人工定义留多少个feature或者算法选取这些feature）
   > （C） 正则化（留下所有的feature，但对于部分feature定义其parameter非常小）
   > （D） 交叉验证

### 参考

1. 支持向量机通俗导论（理解SVM的三层境界）http://blog.csdn.net/v_july_v/article/details/7624837
2. [LR与SVM的异同](http://www.cnblogs.com/zhizhan/p/5038747.html)
3. 《机器学习》周志华老师
