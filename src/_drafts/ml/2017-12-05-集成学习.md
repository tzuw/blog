---
layout:  post
title: "集成学习" 
date:   2019-08-11 22:47:42                    
author:  "tzuw"
header-img: "img/post-bg-2015.jpg"
catalog:   false
tags: 機器學習
---
##### 基础稍差之给我一点一点补齐

- [ ] 关键字提取，tf-idf python实现
- [ ] 丢鸡蛋问题 -》动态规划
- [x] Logistic Regression
- [ ] 集成学习；Boosting：Adaboost，GBDT；Bagging：随机森林

## 集成学习

`梯度提升树的别名，Mulitiple Additive Regression Tree，Tree Net。`

前言：

（1）弱分类器和强分类器在计算学习理论上是等价的，所以就产生了把弱分类器变成强分类器的一系列算法。

（2）在深度学习还没开始火之前，**梯度提升树**和**支持向量机**是两个在业界效果不错的算法。

首先，理清观念。提升方法，“从弱学习方法出发，反复学习，得到一系列的弱分类器，然后组合这些弱分类器变成一个强分类器。”

#### 零、Adaboost

一口述：使得上一轮中分类错误的样本能够在下一轮训练分类器的时候获得更大的关注。训练t轮学习算法，产生t个弱分类器。最后，经过加法模型组成一个强分类器。

Adaboost思想：通过决策树调整数据权重，新的模型训练调整后的数据，用来找到对误分类的数据准确度更高的模型

**（Adaboost的思想可以被应用在很多学习算法中）**

（提高前一轮弱分类器错误分类的样本权值，降低分类正确的样本权值）

#### 壹、提升树

拟合上一步得到的*强分类器*的残差，训练一个决策树，来逼近标签值。

#### 贰、梯度提升树

负梯度拟合！负梯度拟合！负梯度拟合！

GBDT的思想：一步步缩小估计值与标签的误差。

“利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差近似值，拟合一个回归树。”

~~数据权值没有什么变化。~~

#### 叄、随机森林   [Breiman, 2001a]

#### 伍、疑问

1. 怎么理解前向分布算法是一个贪心算法？
2. 既然adaboost和提升树都是要学习下一个基本分类器的参数，以及基本分类器在加法模型的权值，两个有什么差别呢？和梯度提升树又有什么差别，梯度两个字体现在哪里？

笔记参考：

《统计学习方法》李航

[梯度提升树原理小结](http://www.cnblogs.com/pinard/p/6140514.html)  刘建平[Pinard]

---

### **下面是还没整理的部分**

从Adaboost理解，关键的步骤在

- 学习（更新）训练集的样本权重，学习一个新的加权后的分步；
- 学习（更新）基础分类器在加法模型中的权重；

抽象的来说有三个大步骤：

（1）保证基本分类器能够在原始数据上学习

​	白话，假设每个训练样本在学习分类器时同等重要。即是，假设训练数据具有相同的权值分步。

（2）反复学习基本分类器m轮

​	学习包括：原始训练数据在各个基本分类其中的权重，各个基本分类器在下一步线性组合时的权重。

​	子步骤如下：

​		i. 使用当前的分步加权原始训练数据，学习基本分类器

​		ii. 计算基本分类器在当前加权训练集上的分类误差率

​		iii. 计算各个基本分类器在线性组合时的权重

​		iv. 更新训练数据的权值为下一步骤做准备

（3）线性组合反复学习之后的基本分类器构建最终的集成分类器

> <!-比较模糊的地方是：（1）指数损失函数那边需要加强一下；（2）规范化因子->

> “Adaboost是加法模型，损失函数为指数函数                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ，学习算法为<u>前向分步算法</u>时的二分类学习法。”

**用<u>前向分步算法</u>来解释Adaboost。**

在给定训练数据和损失函数***L(y,f(x))***的条件下，学习加法模型成为**经验风险极小化**，是一个复杂的优化问题。<u>前向分步算法</u>用与讨论这一问题。

**前向分步算法**：旨在将同时求解**<u>所有</u>**基础分类器参数和它们在线性组合时的权重这个问题，转换成**<u>一次只求解一个</u>**基础分类器的参数和它的线性组合权重。假设有m个分类器，每次只更新加法模型中的一个基础分类器，而不去动之前已经更新好的m-1个分类器。
