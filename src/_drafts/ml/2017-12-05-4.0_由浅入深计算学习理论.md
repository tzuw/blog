---
layout:  post
title: "由浅入深计算学习理论" 
date:   2019-08-11 22:47:42                    
author:  "tzuw"
header-img: "img/post-bg-2015.jpg"
catalog:   false
tags: 機器學習
---

## 由浅入深计算学习理论  Computational learning theorey

## 1. 浅；大框架

1. **计算学习理论**：通过计算来进行学习的理论。分析学习任务的困难程度，为学习算法提供理论保障，并且根据分析结果知道算法设计。

2. **假设空间/模型**：是一组函数的集合，*f : X -> y* 。函数们映射的准确性却大不相同，统计学习的目标就是从假设空间中选取最优的模型，该模型能**尽可能准确的**将输入空间映射到输出空间。

3. **泛化误差/经验误差（也称训练误差）**

   ![泛化误差](../pics/泛化误差.PNG)![经验误差](../pics/经验误差.PNG)

4. **计算学习理论**最基础的就是PAC Learning Theorey，概率近似正确地学习理论。它描述的是某学习算法能以多大概率地多接近目标概念，并且在该种情形下需要多少训练样例才能达到学习目的。（也就是样本复杂度）简单的用数学表示：

   ![pac可辨识](../pics/pac可辨识.PNG)

   ![PAC可学习](../pics/PAC可学习.PNG)

   解读：某个函数和目标概念的差距小于等于*ϵ*的概率等于1-δ，并且通过推导可以知道其所需的样例数目m。

5. **VC维**：描述假设空间的表示能力/复杂度。VC维越大说明模型的表示能力越强，同时要从中找到最优假设的难度也越大。

6. **Rademacher复杂度**：相比较于VC维，考虑了数据分布

7. **泛化误差界**：量化在最坏的情况下，一个学习算法的表现。上面两种描述假设空间复杂度的方法，都可以定义泛化误差界，其中VC维的泛化误差界相较于Rademacher没有考虑数据分布的条件，VC维得到的泛化误差界比较松。两种界限后面再深入讨论。

8. **结构风险最小化 SRM**：置信风险+经验风险，前者是分类器对未知样本分类时得到的误差，后者是对训练样本。机器学习的目标就是保证结构风险最小化，需要同时降低置信风险和经验风险，要达到这个目的，需要在保证经验风险的同时降低VC维。分类间隔越大，模型VC维越小。##这里想到了SVM

当然不止这些，现实的情况需要考虑很多其他的东西。

## 2. 考虑现实中的情况

现实情况目标概念可能根本不在假设空间中，并且假设空间通常都是无限大的。

- 考虑有限假设空间下的两种情况：目标概念在/不在假设空间中。
- 无限假设空间，常见办法：考虑假设空间的VC维

## 3.深入推导

- 不可分假设空间对PAC可学习理论的推导
- （12.6）决策树分类器的假设空间VC维可以是无穷大
- （12.7）最近邻分类器的假设空间VC维可以是无穷大



