---
layout:  post
title: "聚类算法学习" 
date:   2019-08-11 22:47:42                    
author:  "tzuw"
header-img: "img/post-bg-2015.jpg"
catalog:   false
tags: 聚类
categories: [Machine Learning]
---
## 聚类算法学习

```
一. 聚类算法的一般步骤：
    1. 特征选择
    2. 相似性度量选择
    3. 聚类算法
    4. 结果验证
二. 实际操作时遇到的问题
    1.调参K-means n_clusters
```

### 特征选择

避免冗余不正确的特征，实际测试聚类算法效果时，添加不正确的特征会影响聚类效果。

### 相似性度量

> 衡量两个观测点距离的度量

-  距离 (Minkovski  distance)
  -  曼哈顿距离 ( Manhattan distance ) L1 norm
  -  欧几里得距离 ( Eculidean distance ) L2 norm
  -  切比雪夫距离 (Chebyshev distance ) L∞ norm


- 相似系数 ( Similarity Parameter )
  - 余弦相似性
  - 应用非常广泛，主要优势是不受原线性变换影响，而且可以轻松的转换为距离。
  - 运算速度要比上面的举例法慢得多，特别是当数据的维度很高的时候。

### 聚类算法

- **划分聚类**

  - K-means

  - > 想象将所有观测点聚集成不同的类别，使得同一类别内的观测点相似性高，不同类别观测点的相似性低。

    - 一般步骤：
      1. 通过尝试或先验知识猜测数据集大概存在k个类
      2. 随机选定k个观测点当作初始的聚簇中心
      3. 计算数据集内所有观测点到这k个聚簇中心的距离，将观测点标注成距离其最近的聚簇中心的类别
      4. 通过各观测点的标注重新计算各聚簇的中心位置
      5. 重复上面两步骤直到各个聚簇中心的位置不再发生变化；或者对于非凸的数据集，迭代次数达到预先设定的最大值
    - 优点：
      - 容易理解
      - 时间复杂度低
    - 缺点
      - 需要设定k值；或者确定k值
      - 主要识别圆形簇，球型簇；无法处理环形或者其他形状
      - 不能处理具有离散特征的数据，只能处理连续型的特征数据（解决：k-modes）
      - 对噪声或异常值非常敏感（解决：k-medians）
      - 对非凸数据的处理不好（解决：kernal k-means）

- **密度聚类**

  - DBSCAN	

  - > - 简单来说就是画圈，需要决定圆圈的大小还有圆圈内最少要有几个点。
    > - 一种基于密度的空间聚类算法。该算法将具有足够密度的区域划分为簇，并在具有噪声的空间数据库中发现任意形状的簇，它将簇定义为密度相连的点的最大集合。
    > - 由密度可达关系导出的最大密度相连的样本集合，即为我们最终聚类的一个类别。

    - 一般步骤：
      1. 遍历数据集计算得所有的核心对象
      2. 随机选取一个核心对象，遍历该核心对象的ε邻域内的所有样本记录它们**密度可达**的所有其他样本点
      3. 从第一步骤的核心对象集合中删除已经在第二步骤遍历到的样本点
      4. 重复第二步骤，直到核心对象集合为空
    - 优点
      - 可以发现任意形状的聚簇，相比于K-means只适用于凸数据集或者说圆形/球形的聚簇
      - 对异常点不敏感，反而还可以顺带找出异常点
      - 不用猜测有多少聚簇
    - 缺点
      - 算法假设数据集密度均匀
      - 数据集较大时，时间复杂度高
      - 需要调整参数距离阈值ε，邻域样本数阈值MinPts；不同参数会很大程度影响聚类结果，相比较K-means较复杂

- **层次聚类**

  - Agglomerative Nesting / Divisive

    > 一般分为两种：自底向上和自顶向下，分别记为凝聚层次聚类和分裂层次聚类

    - 凝聚层次聚类

      >  **AGglomerative NESting 介绍**
      >
      >  **一般步骤：**
      >
      >  - **将每个样本点看作一个聚簇**
      >  - **计算所有样本点（集合）两两之间的距离**
      >  - **合并距离最近的两个样本点（集合），将它们划分为同一聚簇**
      >  - **重复执行步骤二，步骤三；直到所有样本属于同一聚簇，或者聚簇数达到预设值**

    - 分裂层次聚类

      > **Hierarchical K-means 介绍**
      >
      > **一般步骤：**
      >
      > - **将所有样本点视为同一聚簇**
      > - **使用K-means算法将样本点划分成K个子簇**
      > - **对于步骤二产生的K个子簇递归的使用K-means算法划分成更小的子簇，直到每个簇只包含一个样本点**

    - 进阶聚类算法

    - > Brich (Balanced Iterative Reducingand Clustering Using Hierarchies)
      >
      > - 利用层次方法的平衡迭代规约和聚类
      > - numerical attribute
      >
      > ROCK (A Hierarchical ClusteringAlgorithm for Categorical Attributes)
      >
      > - categorical attribute
      >
      > Chameleon (A Hierarchical Clustering AlgorithmUsing Dynamic Modeling)
      >
      > - linkage = knn  # linkage指的是计算距离的方法
      > - 聚类效果强大，时间复杂度高

  - 树状图 Dendrogram

  - > 层次聚类的结果常用树状图来表示

  - 联系程度度量（Linkage criteria）

    - 最小距离 single linkage

    - 最大距离 complete linkage

    - 平均距离 average linkage

    - 中心距离 centroid linkage

    - 离差平方和法 ward method

    - > 分别计算类 i和 j 内各点到其重心(均值) 的平方欧式距离和(称为离差平方和), 分别记为 WIi和 Wj ; 然后将所有点合并为一个类 m, 计算其离差平方和 Wm, 最后定义类 i 和 j 之间的平方距离为
      >
      > ***D(i,j) = Wm − Wi − Wj***
      >
      > **离差平方和法**使得两个大的类倾向于有较大的距离，因而不易合并；相反，两个小的类却因倾向于有较小的距离而易于合并。这往往符合我们对聚类的实际要求。
