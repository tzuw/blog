---
layout:  post
title: "估计概率"
date:   2019-08-11 22:47:42                    
author:  "tzuw"
header-img: "img/post-bg-2015.jpg"
catalog:   false
categories: [Machine Learning]
---
## 估计概率

#### Estimating Probability

很多机器学习的方法都依赖于概率逼近。原因很简单，因为学习某个目标函数 *f:X -> Y* ，和学习某个概率函数*P(Y|X)*的意思基本相同。

这份笔记的内容：

1. 联合概率

2. 从训练数据学习/估计概率分布

   - 极大似然估计  Maximum Likelihood Estimation
   - 极大后验概率  Maximum a posterior Estimation


------

### 联合概率    Joint Probability Distributions

> 联合概率是建立一个基于概率的模型的关键。

(Probabilistic model) 举例笔记上的例子而言：

我们可以计算：

- 某样本的某一个变量会取某一个值的概率  P(Gender = male) = 0.6685
- 某样本的某些变量同时会去某些值得概率  P(Wealth=rich∧Gender=female) = 0.0362
- 某样本的条件概率 P(Y|X) = P(X ∧Y)/P(X)

![estimate-probability-example](../pics/estimate-probability-example.png)

换句话说，只要有所有变量的联合分布，就可以计算任何变量子集的**条件概率**或**联合概率**。理论上，我们可以通过这样估计概率的方法来解决任何基于这些变量的**分类**，**回归**，或其他的**函数近似**问题。用上面的例子来说就是，如果我们想要根据一个人工作的时长和性别来估计这个人的收入状况，那么我们可以很简单的去估计一个条件概率P(Wealth|HoursWorked∧Gender)。

但是，上面的状况在现实生活中是不切实际的，通常一个人会有数百个特征，也就是数百个变量。那么在基于有限样本训练直接估计联合概率，在计算上将会遭遇组合爆炸的问题，在数据上将会遭遇样本稀疏问题；特征数越多，问题越严重。举例来说，如果有100个特征，那么要顾及所有变量的联合分布，就需要估计2^100个变数（相当于10^30）；这个数字比地球上的人口还要大，同时也意味着，当我们按照这样的方式来估计联合概率将会有很多的联合概率取值为零。然而这些取值为零的联合概率可能是未被观测到的，因为这些情况的取值在样本训练集中并并没有出现。于是，产生了下面两种概率估计的方法：**极大似然估计**，**极大后验估计**。

两种方法各用一句话来概括：

- 极大似然估计

  ​

- 极大后验估计

